---
title: "Marl Flow (AI Language Trainer)"
description: "Entrenador de fluidez interactivo impulsado por IA que simula contextos reales y se adapta al nivel del usuario"
date: 2026-02-05
draft: false
stack: ["Vue.js", "Flask", "Docker", "Ollama", "Python"]
status: "wip"
type: "AI_TRAINER"
progress: 3
repository_url: "https://github.com/tu-usuario/marl-flow"
images: ["/collibri.svg"]
---

# üó£Ô∏è Marl Flow (AI Language Trainer)

**Categor√≠a:** Software / IA / Educaci√≥n
**Estado:** En desarrollo (Dockerizado y funcional en local)
**Stack:** Vue.js, Flask, Docker, Ollama (Local LLM)

---

## üéØ El Problema (Pain Point)
A pesar de mantener rachas constantes en apps como Duolingo y tomar clases tradicionales, persiste la dificultad para **articular ideas complejas** en ingl√©s. Existe una brecha entre "saber ingl√©s" y sentirse c√≥modo traduciendo pensamientos abstractos o t√©cnicos del espa√±ol al ingl√©s en tiempo real.

## üí° La Soluci√≥n
Un entrenador de fluidez interactivo impulsado por IA que simula contextos reales y se adapta al nivel del usuario.

### ‚öôÔ∏è Mec√°nica (Core Loop)
1.  **Selecci√≥n de Contexto:** El sistema ofrece 3 escenarios distintos (ej. Profesional, Casual, T√©cnico) generados al vuelo por IA.
2.  **Desaf√≠o:** Se presenta una frase o idea en espa√±ol ajustada al nivel actual.
3.  **Input:** El usuario escribe la traducci√≥n o interpretaci√≥n en ingl√©s.
4.  **Feedback Inmediato:** El sistema analiza la respuesta, corrige errores y asigna una calificaci√≥n.
5.  **Progresi√≥n Din√°mica:**
    *   Sesiones de 10 rondas.
    *   **Ajuste de Nivel:** Al final de la sesi√≥n, si el desempe√±o es alto, la dificultad aumenta. Si es bajo, se reduce.

## üõ†Ô∏è Arquitectura y "Fanfarroner√≠a T√©cnica"
Este proyecto es una demostraci√≥n de arquitectura moderna y soberan√≠a de datos.

*   **Frontend:** **Vue.js + Vite**. Interfaz reactiva y ligera.
*   **Backend:** **Python (Flask)**. Gestiona la l√≥gica de estado (sesi√≥n, puntaje, nivel actual) y sirve como API Gateway para el modelo.
*   **Infraestructura:** **Docker Compose**. El sistema completo (Front + Back + DB) vive en contenedores aislados.
*   **Hardware:** Optimizado para correr *on-premise* en una **Raspberry Pi**.
*   **Motor de IA:** **Ollama**. Todo el procesamiento de lenguaje ocurre localmente, garantizando privacidad y cero latencia de red externa.

### üíç Meta-Prompting ("Un Prompt para gobernarlos a todos")
El desarrollo de este proyecto fue asistido por una **Meta-Gem** (Gema personalizada en Gemini) dise√±ada espec√≠ficamente para actuar como "Director de Producto".
Esta Gema no escribi√≥ c√≥digo ciegamente, sino que **dise√±√≥ los prompts** que controlan cada aspecto del sistema:
*   **Branding:** Iteraci√≥n de nombres y prompts para generaci√≥n de Logo.
*   **UI/UX:** Creaci√≥n de instrucciones precisas para que herramientas como Google Stitch generaran la interfaz.
*   **L√≥gica de Negocio:** Redacci√≥n y ajuste fino de los prompts del sistema (Contexto, Dificultad, Juez) para asegurar consistencia en el LLM local.

## üß† Aprendizajes
*   Desplegar LLMs en hardware limitado (RPi) requiere optimizaci√≥n extrema.
*   La IA no solo sirve para escribir c√≥digo, sino para orquestar la visi√≥n del producto completo.
